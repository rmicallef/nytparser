{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYT Article Data Parser\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# pull in the data from the Boydstun study\n",
    "data_path = '/home/ryan/work/nytparser/data/'\n",
    "data_file = 'Boydstun_random_100.csv'\n",
    "#data_file = 'Boydstun_NYT_FrontPage_Dataset TEST9Items.csv'\n",
    "data_location = data_path + data_file\n",
    "\n",
    "# topic codes used in the Boydstun study\n",
    "boydstun_topic_codes = {'1':'Macroeconomics',\n",
    "                        '2':'Civil Rights, Minority Issues, and Civil Liberties',\n",
    "                        '3':'Health',\n",
    "                        '4':'Agriculture',\n",
    "                        '5':'Labor, Employment, and Immigration',\n",
    "                        '6':'Education',\n",
    "                        '7':'Environment',\n",
    "                        '8':'Energy',\n",
    "                        \"9\":'Quality of Life',\n",
    "                        '10':'Transportation',\n",
    "                        '12':'Law, Crime, and Family Issues',\n",
    "                        '13':'Social Welfare',\n",
    "                        '14':'Community Development and Housing Issues',\n",
    "                        '15':'Banking, Finance, and Domestic Commerce',\n",
    "                        '16':'Defense',\n",
    "                        '17':'Space, Science, Technology and Communications',\n",
    "                        '18':'Foreign Trade',\n",
    "                        '19':'International Affairs and Foreign Aid',\n",
    "                        '20':'Government Operations',\n",
    "                        '21':'Public Lands and Water Management',\n",
    "                        '24':'State and Local Government Administration',\n",
    "                        '26':'Weather and Natural Disasters',\n",
    "                        '27':'Fires',\n",
    "                        '28':'Arts and Entertainment',\n",
    "                        '29':'Sports and Recreation',\n",
    "                        '30':'Death Notices',\n",
    "                        '31':'Churches and Religion',\n",
    "                        '99':'Other, Miscellaneous, and Human Interest',\n",
    "                       }\n",
    "    \n",
    "columns = defaultdict(list)\n",
    "\n",
    "with open(data_location, newline='') as csvfile:\n",
    "    rdr = csv.DictReader(csvfile)\n",
    "    for row in rdr:\n",
    "        for(k,v) in row.items():\n",
    "            columns[k].append(v)\n",
    "\n",
    "boydstun_article_titles = columns['title']\n",
    "boydstun_article_classes = columns['topic_2digit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "# Prepare article titles for search \n",
    "       \n",
    "URL_encoded_titles = []\n",
    "\n",
    "for title in boydstun_article_titles:\n",
    "    title = \"\\\"\" + title + \"\\\"\"\n",
    "    #print(title)\n",
    "    URL_encoded_title = urllib.parse.quote(title)\n",
    "    #print(URL_encoded_title)\n",
    "    URL_encoded_titles.append(URL_encoded_title)\n",
    "\n",
    "#print(URL_encoded_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 article titles to search for.\n",
      "---> Fetching desriptions and class for article 0\n",
      "---> Fetching desriptions and class for article 1\n",
      "---> Fetching desriptions and class for article 2\n",
      "---> Fetching desriptions and class for article 3\n",
      "---> Fetching desriptions and class for article 4\n",
      "---> Fetching desriptions and class for article 5\n",
      "---> Fetching desriptions and class for article 6\n",
      "---> Fetching desriptions and class for article 7\n",
      "---> Fetching desriptions and class for article 8\n",
      "---> Fetching desriptions and class for article 9\n",
      "---> Fetching desriptions and class for article 10\n",
      "---> Fetching desriptions and class for article 11\n",
      "---> Fetching desriptions and class for article 12\n",
      "---> Fetching desriptions and class for article 13\n",
      "---> Fetching desriptions and class for article 14\n",
      "---> Fetching desriptions and class for article 15\n",
      "---> Fetching desriptions and class for article 16\n",
      "---> Fetching desriptions and class for article 17\n",
      "---> Fetching desriptions and class for article 18\n",
      "---> Fetching desriptions and class for article 19\n",
      "---> Fetching desriptions and class for article 20\n",
      "---> Fetching desriptions and class for article 21\n",
      "---> Fetching desriptions and class for article 22\n",
      "---> Fetching desriptions and class for article 23\n",
      "---> Fetching desriptions and class for article 24\n",
      "---> Fetching desriptions and class for article 25\n",
      "---> Fetching desriptions and class for article 26\n",
      "---> Fetching desriptions and class for article 27\n",
      "---> Fetching desriptions and class for article 28\n",
      "---> Fetching desriptions and class for article 29\n",
      "---> Fetching desriptions and class for article 30\n",
      "---> Fetching desriptions and class for article 31\n",
      "---> Fetching desriptions and class for article 32\n",
      "---> Fetching desriptions and class for article 33\n",
      "---> Fetching desriptions and class for article 34\n",
      "---> Fetching desriptions and class for article 35\n",
      "---> Fetching desriptions and class for article 36\n",
      "---> Fetching desriptions and class for article 37\n",
      "---> Fetching desriptions and class for article 38\n",
      "---> Fetching desriptions and class for article 39\n",
      "---> Fetching desriptions and class for article 40\n",
      "---> Fetching desriptions and class for article 41\n",
      "---> Fetching desriptions and class for article 42\n",
      "---> Fetching desriptions and class for article 43\n",
      "---> Fetching desriptions and class for article 44\n",
      "---> Fetching desriptions and class for article 45\n",
      "---> Fetching desriptions and class for article 46\n",
      "---> Fetching desriptions and class for article 47\n",
      "---> Fetching desriptions and class for article 48\n",
      "---> Fetching desriptions and class for article 49\n",
      "---> Fetching desriptions and class for article 50\n",
      "---> Fetching desriptions and class for article 51\n",
      "---> Fetching desriptions and class for article 52\n",
      "---> Fetching desriptions and class for article 53\n",
      "---> Fetching desriptions and class for article 54\n",
      "---> Fetching desriptions and class for article 55\n",
      "---> Fetching desriptions and class for article 56\n",
      "---> Fetching desriptions and class for article 57\n",
      "---> Fetching desriptions and class for article 58\n",
      "---> Fetching desriptions and class for article 59\n",
      "---> Fetching desriptions and class for article 60\n",
      "---> Fetching desriptions and class for article 61\n",
      "---> Fetching desriptions and class for article 62\n",
      "---> Fetching desriptions and class for article 63\n",
      "---> Fetching desriptions and class for article 64\n",
      "---> Fetching desriptions and class for article 65\n",
      "---> Fetching desriptions and class for article 66\n",
      "---> Fetching desriptions and class for article 67\n",
      "---> Fetching desriptions and class for article 68\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Get article data from NYT Developer API\n",
    "\n",
    "# Build the query\n",
    "\n",
    "# For reference\n",
    "#base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "#API_key = 'MEt3Ki6vTVUvG4unee31Sb6MuSq1ACVO'\n",
    "# Test URL\n",
    "#url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=MEt3Ki6vTVUvG4unee31Sb6MuSq1ACVO&begin_date=19960101&end_date=20061231&fq=headline:(%22Tax%20Panel%20Says%20Popular%20Breaks%20Should%20Be%20Cut%22)&fq=print_page:1'\n",
    "\n",
    "articles_to_parse = []\n",
    "article_descriptions = []\n",
    "\n",
    "print(\"Found \" + str(len(URL_encoded_titles)) + \" article titles to search for.\")\n",
    "i = 0\n",
    "\n",
    "for title in URL_encoded_titles:\n",
    "    print(\"---> Fetching desriptions and class for article \" + str(i))\n",
    "    url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=MEt3Ki6vTVUvG4unee31Sb6MuSq1ACVO&begin_date=19960101&end_date=20061231&fq=headline:(' + title + ')&fq=print_page:1'\n",
    "    #print(title, url)\n",
    "    response = urllib.request.urlopen(url)\n",
    "    json_dict = json.loads(response.read())\n",
    "    #print(json.dumps(json_dict, indent=4))\n",
    "    articles_to_parse.append(json_dict)\n",
    "    \n",
    "    # Go through articles and extract what we need\n",
    "    for doc in json_dict['response']['docs']:\n",
    "        if 'snippet' in doc:\n",
    "            #print(doc['snippet'])\n",
    "            snippet = doc['snippet']\n",
    "        else:\n",
    "            snippet = ''\n",
    "                \n",
    "        if 'lead_paragraph' in doc:\n",
    "            #print(doc['lead_paragraph'])\n",
    "            lead_paragraph = doc['lead_paragraph']\n",
    "        else:\n",
    "            lead_paragraph = ''\n",
    "\n",
    "        article_description = snippet + lead_paragraph\n",
    "        article_descriptions.append(article_description)\n",
    "    \n",
    "    i += 1\n",
    "    time.sleep(6) #limit enforced by NYT\n",
    "\n",
    "print(\"Articles pulled:\" + str(len(articles_to_parse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to useful format\n",
    "\n",
    "\n",
    "#pickling the imported data so I don't have to wait for it forever again.\n",
    "import pickle\n",
    "\n",
    "pickle_file = open(data_location + '.pkl', 'wb')\n",
    "pickle.dump(article_descriptions, pickle_file)\n",
    "pickle_file.close()\n",
    "\n",
    "\"\"\"\n",
    "Goal output:\n",
    "\n",
    "{\"text\": \"some text\", \"label\": \"politics\"}\n",
    "{\"text\": \"some other text\", \"label\": \"science\"}\n",
    "\"\"\"\n",
    "\n",
    "output_list = []\n",
    "\n",
    "print(len(article_descriptions))\n",
    "\n",
    "\n",
    "for i in range(len(article_descriptions)):\n",
    "    \n",
    "    print('working on article ' + str(i) + ': topic: ' + str(boydstun_topic_codes[boydstun_article_classes[i]]))\n",
    "    article_dict = {}\n",
    "    try:\n",
    "        article_dict['text'] = article_descriptions[i]\n",
    "        article_dict['label'] = str(boydstun_topic_codes[boydstun_article_classes[i]])\n",
    "        output_list.append(article_dict)\n",
    "    except Exception as e:\n",
    "        print(type(e))\n",
    "        \n",
    "        \n",
    "\n",
    "with open(data_path + 'nyt_article_data.json', 'w') as outfile:\n",
    "    json.dump(output_list, outfile, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
